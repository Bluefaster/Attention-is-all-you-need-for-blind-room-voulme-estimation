{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prototyping\n",
    "\n",
    "Building the basis for our model experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T12:04:15.415344Z",
     "start_time": "2023-06-13T12:04:11.987683Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12099\\.conda\\envs\\tensorflow02\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.nn import Conv2d, AvgPool2d, ReLU, Dropout, Flatten, Linear, Sequential, Module\n",
    "from torch.optim import Adam\n",
    "from time import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "MODELS_DIR  = 'C:/Users/12099/1_room/scratch/ci411/sonos_rirs/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T12:04:17.754533Z",
     "start_time": "2023-06-13T12:04:17.743563Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "model_dict['name'] = \"testrun2_regularization\"\n",
    "model_dict['notes'] = \"same as test run but with regularization\"\n",
    "model_dict['data_path'] = 'C:/Users/12099/1_room/scratch/ci411/sonos_rirs/features/080122_5k_phase/feature_df.csv'\n",
    "model_dict['model_path'] = os.path.join(MODELS_DIR, model_dict['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T12:04:21.101711Z",
     "start_time": "2023-06-13T12:04:20.886298Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # 尝试导入IPython\n",
    "    from IPython import get_ipython\n",
    "    # 检查是否在IPython环境下\n",
    "    if get_ipython() is not None:\n",
    "        # 加载autoreload扩展\n",
    "        %load_ext autoreload\n",
    "        # 设置autoreload为2\n",
    "        %autoreload 2\n",
    "except ImportError:\n",
    "    # 如果IPython没有被安装，则不作任何操作\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:03:34.190152Z",
     "start_time": "2023-06-13T12:04:24.220339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Using device: cuda\n",
      "Using device: cuda\n",
      "\n",
      "Overwriting model at  C:/Users/12099/1_room/scratch/ci411/sonos_rirs/models/testrun2_regularization\n",
      "\n",
      "Loading data from  C:/Users/12099/1_room/scratch/ci411/sonos_rirs/features/080122_5k_phase/feature_df.csv\n",
      "Dataset size: (5000, 15)\n",
      "Creating training dataloader\n",
      "Creating validation dataloader\n",
      "Creating test dataloader\n",
      "Feature batch shape: torch.Size([16, 1, 44, 1997])\n",
      "Labels batch shape: torch.Size([16, 1])\n",
      "Beginning training for 25 epochs...\n",
      "E: 0\tD: 3996.39s\tTL: 64774.0468734423\tVL: [46005.45613705]\tVB:[-30.08669857]\tVP: [0.06165927]\tVM: [1.48783043]\tVV:[0.01195742]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1068\\286507823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel_funcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m model_funcs.train_model(model_funcs.Baseline_Model, model_dict,\\\n\u001b[1;32m----> 4\u001b[1;33m                         overwrite=True, epochs=25,log=False)\n\u001b[0m",
      "\u001b[1;32m~\\1_room\\volume-estimation-main\\prototyping_notebooks\\modeling.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_func, model_dict, targets, batch_size, lr_init, l2_reg, overwrite, epochs, log, sched_thres, normalize_targets)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mval_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_eval_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m#update LR scheduler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\1_room\\volume-estimation-main\\prototyping_notebooks\\volume_estimation\\evaluation.py\u001b[0m in \u001b[0;36mcompute_eval_metrics\u001b[1;34m(dataloader, model, log, verbose)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[0mtarget_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[0mpred_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mn_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import modeling as model_funcs\n",
    "model_funcs.train_model(model_funcs.Baseline_Model, model_dict,\\\n",
    "                        overwrite=True, epochs=25,log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:15, 32.93it/s]\n"
     ]
    }
   ],
   "source": [
    "feat_df = pd.read_csv(model_dict['data_path'])\n",
    "model_path = os.path.join(MODELS_DIR, model_dict['name'])\n",
    "\n",
    "dataset = []\n",
    "\n",
    "    \n",
    "def create_dataloader(feature_df, batch_size=1, log=True):\n",
    "    dataset = []\n",
    "    for row in tqdm(feature_df.iterrows()):\n",
    "        feat_file = row[1]['file_feature']\n",
    "        loaded = np.load(feat_file)\n",
    "\n",
    "        feature = loaded['feat']\n",
    "        feature = feature.reshape((1, feature.shape[0], feature.shape[1]))\n",
    "        feature = np.real(feature)\n",
    "\n",
    "        vol = loaded['vol']\n",
    "        if log:\n",
    "            vol = np.log10(vol)\n",
    "        dataset.append((feature, vol))\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataloader(feat_df, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name testrun2_regularization\n",
      "notes same as test run but with regularization\n",
      "data_path /scratch/ci411/sonos_rirs/features/072622_small/feature_df.csv\n",
      "model_path /scratch/ci411/sonos_rirs/models/testrun2_regularization\n"
     ]
    }
   ],
   "source": [
    "savename = './testmodeldict.json'\n",
    "with open(savename, 'w') as f:\n",
    "    json.dump(model_dict, f)\n",
    "    \n",
    "with open(savename) as f:\n",
    "    load_dict = json.load(f)\n",
    "    \n",
    "for key in load_dict.keys():\n",
    "    print(key, load_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "409it [00:00, 440.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:00, 466.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:00, 404.13it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = feat_df[feat_df['split']=='train']\n",
    "val_df = feat_df[feat_df['split']=='val']\n",
    "test_df = feat_df[feat_df['split']=='test']\n",
    "\n",
    "print(\"Creating training dataloader\")\n",
    "train_dataloader = create_dataloader(train_df, batch_size=16)\n",
    "\n",
    "print(\"Creating validation dataloader\")\n",
    "val_dataloader = create_dataloader(val_df)\n",
    "\n",
    "print(\"Creating test dataloader\")\n",
    "test_dataloader = create_dataloader(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([16, 1, 24, 1999])\n",
      "Labels batch shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Baseline_Model(Module):\n",
    "    def __init__(self, input_shape):\n",
    "        #accepts a tuple with the height/width of the feature\n",
    "        #matrix to set the FC layer dimensions\n",
    "        super(Baseline_Model, self).__init__()\n",
    "        \n",
    "        #block1\n",
    "        Conv1 = Conv2d(1, 30, kernel_size=(1,10), stride=(1,1))\n",
    "        Avgpool1 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "        #block2\n",
    "        Conv2 = Conv2d(30, 20, kernel_size=(1,10), stride=(1,1))\n",
    "        Avgpool2 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "        #block3\n",
    "        Conv3 = Conv2d(20, 10, kernel_size=(1,10), stride=(1,1))\n",
    "        Avgpool3 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "        #block4\n",
    "        Conv4 = Conv2d(10, 10, kernel_size=(1,10), stride=(1,1))\n",
    "        Avgpool4 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "        #block5\n",
    "        Conv5 = Conv2d(10, 5, kernel_size=(3,9), stride=(1,1))\n",
    "        Avgpool5 = AvgPool2d((1,2), stride=(1,2))\n",
    "\n",
    "        #block6\n",
    "        Conv6 = Conv2d(5, 5, kernel_size=(3,9), stride=(1,1))\n",
    "        Avgpool6 = AvgPool2d((2,2), stride=(2,2))\n",
    "\n",
    "        #dropout\n",
    "        dropout_layer = Dropout(p=0.5)\n",
    "        height5 = input_shape[0] - 2\n",
    "        height6 = (height5 - 2) // 2\n",
    "\n",
    "        time1 = (input_shape[1] - 9) // 2\n",
    "        time2 = (time1 - 9) // 2\n",
    "        time3 = (time2 - 9) // 2\n",
    "        time4 = (time3 - 9) // 2\n",
    "        time5 = (time4 - 7) // 2\n",
    "        time6 = (time5 - 7) // 2\n",
    "\n",
    "        flat_dims = 5 * height6 * time6\n",
    "        fc_layer = Linear(flat_dims, 1)\n",
    "        \n",
    "        self.net = Sequential(\n",
    "                    Conv1, ReLU(), Avgpool1,\n",
    "                    Conv2, ReLU(), Avgpool2,\n",
    "                    Conv3, ReLU(), Avgpool3,\n",
    "                    Conv4, ReLU(), Avgpool4,\n",
    "                    Conv5, ReLU(), Avgpool5,\n",
    "                    Conv6, ReLU(), Avgpool6,\n",
    "                    dropout_layer, Flatten(),\n",
    "                    fc_layer, Flatten()\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = features.size()[2]\n",
    "input_width = features.size()[3]\n",
    "\n",
    "model = Baseline_Model((input_height, input_width)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 4.7331875006884045, 'bias': -2.123222233206245, 'pearson_cor': 0.1444998490590587, 'mean_mult': 132.80738737171563}\n"
     ]
    }
   ],
   "source": [
    "def MSE(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "def Bias(output, target):\n",
    "    loss = torch.mean(output - target)\n",
    "    return loss\n",
    "\n",
    "def CovStep(output, target, output_mean, target_mean):\n",
    "    loss = torch.mean(((output - output_mean) * (target - target_mean)))\n",
    "    return loss\n",
    "\n",
    "def MeanAbsLogStep(output, target, log=True):\n",
    "    #convert out of log\n",
    "    if log:\n",
    "        vol_pred = 10**output\n",
    "        vol_target = 10**target\n",
    "    else:\n",
    "        vol_pred = output\n",
    "        vol_target = target\n",
    "    loss = torch.mean(torch.abs(torch.log(vol_pred/vol_target)))\n",
    "    return loss\n",
    "\n",
    "def compute_eval_metrics(dataloader, model, log=True):\n",
    "    target_sum = 0\n",
    "    pred_sum = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    for (x,y) in dataloader:        \n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        target_sum += y.sum()\n",
    "        pred_sum += pred.sum()\n",
    "        n_steps += 1\n",
    "    \n",
    "    target_mean = target_sum/n_steps\n",
    "    pred_mean = pred_sum/n_steps\n",
    "    \n",
    "    mse = 0\n",
    "    mean_error = 0\n",
    "    cov = 0\n",
    "    abs_log_ratio = 0\n",
    "    \n",
    "    var_pred = 0 #technically var * N but gets cancelled out in Pearson calculation\n",
    "    var_target = 0 \n",
    "    \n",
    "    for (x,y) in dataloader:        \n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        mse += MSE(pred, y)\n",
    "        mean_error += Bias(pred, y)\n",
    "        cov += CovStep(pred, y, pred_mean, target_mean)\n",
    "        abs_log_ratio += MeanAbsLogStep(pred, y, log=log)\n",
    "        \n",
    "        var_pred += MSE(pred, pred_mean)\n",
    "        var_target += MSE(y, target_mean)\n",
    "        \n",
    "        pears = CovStep(pred, y, pred_mean, target_mean)/(torch.sqrt(MSE(pred, pred_mean))*torch.sqrt(MSE(y, target_mean)))\n",
    "    \n",
    "    out_dict = {}\n",
    "    out_dict['mse'] = (mse / n_steps).item()\n",
    "    out_dict['bias'] = (mean_error / n_steps).item()\n",
    "    out_dict['pearson_cor'] = (cov/(torch.sqrt(var_pred) * torch.sqrt(var_target))).item()\n",
    "    out_dict['mean_mult'] = (torch.exp(abs_log_ratio/n_steps)).item()\n",
    "    \n",
    "    return out_dict\n",
    "    \n",
    "eval_dict = compute_eval_metrics(val_dataloader, model)\n",
    "print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tDuration: 24.69s\tTrain loss: 0.9086\tVal loss: 0.2417\tVal bias:0.1182\tVal Pearson correlation: 1.6877e-01\tVal MeanMult: 2.0553\n",
      "Epoch: 1\tDuration: 25.78s\tTrain loss: 0.2907\tVal loss: 0.2280\tVal bias:0.0105\tVal Pearson correlation: 3.6151e-01\tVal MeanMult: 2.1492\n",
      "Epoch: 2\tDuration: 26.24s\tTrain loss: 0.2826\tVal loss: 0.2304\tVal bias:0.0493\tVal Pearson correlation: 4.3434e-01\tVal MeanMult: 2.0868\n",
      "Epoch: 3\tDuration: 27.05s\tTrain loss: 0.2855\tVal loss: 0.2288\tVal bias:-0.0290\tVal Pearson correlation: 8.3376e-02\tVal MeanMult: 2.2419\n",
      "Epoch: 4\tDuration: 27.38s\tTrain loss: 0.2874\tVal loss: 0.2289\tVal bias:-0.0313\tVal Pearson correlation: 3.4533e-02\tVal MeanMult: 2.2483\n",
      "Epoch: 5\tDuration: 28.66s\tTrain loss: 0.2874\tVal loss: 0.2297\tVal bias:0.0423\tVal Pearson correlation: 4.0567e-02\tVal MeanMult: 2.0966\n",
      "Epoch: 6\tDuration: 28.58s\tTrain loss: 0.2887\tVal loss: 0.2280\tVal bias:0.0044\tVal Pearson correlation: 4.4734e-02\tVal MeanMult: 2.1609\n",
      "Epoch: 7\tDuration: 28.54s\tTrain loss: 0.2760\tVal loss: 0.2281\tVal bias:0.0129\tVal Pearson correlation: 4.4734e-02\tVal MeanMult: 2.1445\n",
      "Epoch: 8\tDuration: 28.49s\tTrain loss: 0.2853\tVal loss: 0.2290\tVal bias:-0.0330\tVal Pearson correlation: -5.5811e-16\tVal MeanMult: 2.2528\n",
      "Epoch: 9\tDuration: 28.44s\tTrain loss: 0.2766\tVal loss: 0.2289\tVal bias:0.0306\tVal Pearson correlation: -6.0664e-16\tVal MeanMult: 2.1139\n",
      "Epoch: 10\tDuration: 28.56s\tTrain loss: 0.2813\tVal loss: 0.2280\tVal bias:-0.0099\tVal Pearson correlation: -6.2012e-16\tVal MeanMult: 2.1918\n",
      "Epoch: 11\tDuration: 28.40s\tTrain loss: 0.2784\tVal loss: 0.2280\tVal bias:0.0095\tVal Pearson correlation: 6.0664e-16\tVal MeanMult: 2.1511\n",
      "Epoch: 12\tDuration: 28.35s\tTrain loss: 0.2673\tVal loss: 0.2296\tVal bias:0.0409\tVal Pearson correlation: 6.2012e-16\tVal MeanMult: 2.0986\n",
      "Epoch: 13\tDuration: 28.40s\tTrain loss: 0.2857\tVal loss: 0.2280\tVal bias:-0.0049\tVal Pearson correlation: -1.0483e-01\tVal MeanMult: 2.1806\n",
      "Epoch: 14\tDuration: 28.37s\tTrain loss: 0.2776\tVal loss: 0.2284\tVal bias:0.0226\tVal Pearson correlation: -8.9206e-02\tVal MeanMult: 2.1258\n",
      "Epoch: 15\tDuration: 28.33s\tTrain loss: 0.2777\tVal loss: 0.2283\tVal bias:0.0198\tVal Pearson correlation: -6.2012e-16\tVal MeanMult: 2.1312\n",
      "Epoch: 16\tDuration: 28.32s\tTrain loss: 0.2717\tVal loss: 0.2298\tVal bias:0.0435\tVal Pearson correlation: 6.0664e-16\tVal MeanMult: 2.0948\n",
      "Epoch: 17\tDuration: 28.33s\tTrain loss: 0.2748\tVal loss: 0.2302\tVal bias:0.0480\tVal Pearson correlation: 6.0664e-16\tVal MeanMult: 2.0883\n",
      "Epoch: 18\tDuration: 28.25s\tTrain loss: 0.2806\tVal loss: 0.2287\tVal bias:0.0275\tVal Pearson correlation: 6.0664e-16\tVal MeanMult: 2.1185\n",
      "Epoch: 19\tDuration: 28.25s\tTrain loss: 0.2723\tVal loss: 0.2303\tVal bias:0.0487\tVal Pearson correlation: -6.0664e-16\tVal MeanMult: 2.0875\n",
      "Epoch: 20\tDuration: 29.00s\tTrain loss: 0.2780\tVal loss: 0.2287\tVal bias:0.0275\tVal Pearson correlation: 5.5811e-16\tVal MeanMult: 2.1185\n",
      "Epoch: 21\tDuration: 28.69s\tTrain loss: 0.2681\tVal loss: 0.2290\tVal bias:0.0321\tVal Pearson correlation: nan\tVal MeanMult: 2.1117\n",
      "Epoch: 22\tDuration: 28.49s\tTrain loss: 0.2802\tVal loss: 0.2301\tVal bias:0.0462\tVal Pearson correlation: 6.0664e-16\tVal MeanMult: 2.0909\n",
      "Epoch: 23\tDuration: 28.38s\tTrain loss: 0.2804\tVal loss: 0.2356\tVal bias:0.0874\tVal Pearson correlation: 5.5702e-02\tVal MeanMult: 2.0603\n",
      "Epoch: 24\tDuration: 28.15s\tTrain loss: 0.2808\tVal loss: 0.2308\tVal bias:0.0537\tVal Pearson correlation: 3.7316e-02\tVal MeanMult: 2.0823\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(model.parameters(),lr=0.005, weight_decay=1e-2)\n",
    "\n",
    "hist = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"val_loss\": [],\n",
    "    \"val_bias\": [],\n",
    "    \"val_pearson_cor\": [],\n",
    "    \"val_mean_mult\": []\n",
    "}\n",
    "\n",
    "for ep in range(25):\n",
    "    t_start = time()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_steps = 0\n",
    "    val_steps = 0\n",
    "    \n",
    "    for (x, y) in train_dataloader:\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        loss = MSE(pred, y.reshape((y.shape[0], 1)))\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss\n",
    "        train_steps += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        val_metrics = compute_eval_metrics(val_dataloader, model)\n",
    "    \n",
    "    \n",
    "    hist['train_loss'].append(train_loss/train_steps)\n",
    "    hist['val_loss'].append(val_metrics['mse'])\n",
    "    hist['val_bias'].append(val_metrics['bias'])\n",
    "    hist['val_pearson_cor'].append(val_metrics['pearson_cor'])\n",
    "    hist['val_mean_mult'].append(val_metrics['mean_mult'])\n",
    "    \n",
    "    t_end = time()\n",
    "    \n",
    "    t_elapsed = t_end - t_start\n",
    "    print(\"Epoch: {}\\tDuration: {:.2f}s\\tTrain loss: {:.4f}\\tVal loss: {:.4f}\\tVal bias:{:.4f}\\tVal Pearson correlation: {:.4e}\\tVal MeanMult: {:.4f}\"\\\n",
    "          .format(ep, t_elapsed, train_loss/train_steps, val_metrics['mse'],\\\n",
    "                  val_metrics['bias'], val_metrics['pearson_cor'],val_metrics['mean_mult']))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.04718757373347041, 'bias': 0.21722700967759592, 'pearson_cor': -4.86056656393074e-08, 'mean_mult': 1.6490241262038319}\n"
     ]
    }
   ],
   "source": [
    "eval_test = compute_eval_metrics(test_dataloader, model)\n",
    "print(eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.keys()\n",
    "np.std(hist['val_loss'][15:])\n",
    "np.arange(100)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 423.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 3.5533725188865946, 'bias': -1.7790347908965678, 'pearson_cor': -0.16924566496616403, 'mean_mult': 60.12218988391525}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1847749759571498\n",
      "2.1847749743258196\n",
      "2.1847749763467244\n",
      "2.1847749806196854\n",
      "2.1847749823384\n",
      "2.184774982408507\n",
      "2.184774982342067\n",
      "2.1847749887373906\n",
      "2.1847749870368007\n",
      "2.1847749831466228\n",
      "2.1847749808330774\n",
      "2.1847749857464227\n",
      "2.1847749892471793\n",
      "2.184774983813266\n",
      "2.1847749695552214\n",
      "2.1847749768380846\n",
      "2.1847749785676336\n",
      "2.184774979268998\n",
      "2.1847749923730744\n",
      "2.1847749881828107\n"
     ]
    }
   ],
   "source": [
    "random_df = feat_df.sample(20)\n",
    "\n",
    "random_dataloader = create_dataloader(random_df)\n",
    "eval_random = compute_eval_metrics(random_dataloader, model)\n",
    "print(eval_random)\n",
    "\n",
    "for (x, y) in random_dataloader:\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        print(pred.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class print_Model(Module):\n",
    "    def __init__(self, seq):\n",
    "        super(print_Model, self).__init__()\n",
    "        self.net = seq\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Start\\n{}\".format(x.size()))\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "            print(layer)\n",
    "            print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse 45796.25260507299\n",
      "bias 208.8553279556989\n",
      "pearson_cor 0.11266241132024864\n",
      "mean_mult 104.27601688167738\n",
      "310.9582128449743\n",
      "352.9361237221425\n",
      "66.39532032311823\n",
      "273.46949087948565\n",
      "203.54653305300818\n",
      "9.16723572200408\n",
      "-2.562087825488065\n",
      "22.972772472801477\n",
      "165.31293551814673\n",
      "150.62623389263476\n",
      "318.6727226084113\n",
      "98.86285874814143\n",
      "-29.583824657848435\n",
      "348.4388819273985\n",
      "236.10297180957184\n",
      "129.09428485168146\n",
      "139.32949064073074\n",
      "259.44537729097897\n",
      "149.21884106490478\n",
      "205.97660735221262\n"
     ]
    }
   ],
   "source": [
    "random_df = feat_df.sample(20)\n",
    "\n",
    "random_dataloader = create_dataloader(random_df, log=False)\n",
    "\n",
    "\n",
    "load_model = Baseline_Model((input_height, input_width)).to(device)\n",
    "model_name = 'testrun4_nonlog'\n",
    "load_model.load_state_dict(torch.load(os.path.join(MODELS_DIR,model_name,'model_state.pt'), map_location=torch.device('cpu')))\n",
    "\n",
    "load_metrics = compute_eval_metrics(test_dataloader, load_model, log=False)\n",
    "for key in load_metrics.keys():\n",
    "    print(key, load_metrics[key])\n",
    "\n",
    "for (x, y) in random_dataloader:\n",
    "    (x, y) = (x.to(device), y.to(device))\n",
    "    pred = load_model(x)\n",
    "    print(pred.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_df['vol'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow02",
   "language": "python",
   "name": "tensorflow02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
